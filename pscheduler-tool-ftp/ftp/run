#!/usr/bin/python

#
# Development Order #5:
#
# This is the meat and bones of the tool, where the actual desired
# commands or operation will be run. The results are then recorded
# and added to the 'results' JSON data, which will then be sent
# back to the test. Both system and api are able to be used here.
#

import os
import sys
import time
import json
import datetime
import subprocess

import pscheduler

# from stdin
input = pscheduler.json_load(exit_on_error=True)
log = pscheduler.Log(prefix='ftp', quiet=True)

# Take input from test spec
try:
    dest = input['test']['spec']['dest']

    source = input['test']['spec']['source']
    
    # The directory to put the new copy of file (optional)
    dest_path = input['test']['spec']['dest-path']

    # Delete transfered file after transfer if cleanup == True
    cleanup = input['test']['spec']['cleanup']

except KeyError as e:
    print(e);	pscheduler.fail('Missing data in input')

#duration = input['test']['spec'].get('duration')#, 'PT10S')

#duration = pscheduler.timedelta_as_seconds( pscheduler.iso8601_as_timedelta(duration) ) 


timeout_iso = input['test']['spec'].get('timeout', 'PT10S')

#log.debug('Timeout ' + str(timeout_iso))

timeout = pscheduler.timedelta_as_seconds( pscheduler.iso8601_as_timedelta(timeout_iso) )
start_time = datetime.datetime.now()
succeeded = False
error = ''
diags = ''

# Run the actual task here:
STDERR = ""
if True:
  # Authentication
  argv = ['curl',
          '-o',   # Flag to output file
          dest,   # Download location
	  source] # Source URL
  
  status, stdout, stderr = pscheduler.run_program(argv, timeout=timeout)

  #log.debug("%s", stderr)

  lines = stderr.split('\r')
  log.debug(str(lines))

  parsed = [stat for stat in lines[-1].split(' ') if stat != '']
  log.debug(str(parsed))
  
  STDERR = parsed

  if status:
    succeeded = False
    error = "Error running program:\n%s"% stderr.strip('\n')

    # Determine how much (if any) of the file was transferred
    if os.path.isfile(dest):
      bytes_sent = os.stat(dest).st_size
      error += '\n Bytes transfered %s'% bytes_sent

  else:
    succeeded = True
    diags = stdout

    if cleanup is True:
      if os.path.isfile(dest): os.remove(dest)

end_time = datetime.datetime.now()

# Organize results into json data
final_results = {
    'succeeded': succeeded,
    'error': error,
    'diags': diags }

results = {'schema': 1, 
	   'succeeded': succeeded}

results['time'] = pscheduler.timedelta_as_iso8601( end_time - start_time)

with open('/tmp/run.log', 'wb') as f:
  f.write('LOGGING ftp/run')
  f.write(str(STDERR))

#%#%#%#%#%#
trans_speed = STDERR[-1].split('\n')[0]

if trans_speed[-1] == 'k':
  #STDERR[6] = int(STDERR[6][:-1]) * 1000# Average Download Speed
  trans_speed = int(trans_speed[:-1]) * 1000

else:# TODO: parsing for MB and greater transfers
  trans_speed = 10

#%#%#%#%#%#
results['throughput'] = int(trans_speed)

if STDERR[1][-1] == 'k':
  STDERR[1] = int(STDERR[1][:-1]) * 1000

elif type(STDERR) != int and STDERR[1][-1] == 'M':
  STDERR[1] = int(STDERR[1][:-1].split('.')[0]) * 1000 * 1000

#%#%#%#%#%#
results['bytes-sent'] = int(STDERR[1])

final_results['result'] = results
pscheduler.succeed_json(final_results)



#?
